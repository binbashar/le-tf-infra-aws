name: AI-Powered Infrastructure Validation

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'apps-devstg/us-east-1/secrets-manager/**/*.tf'
      - 'apps-devstg/us-east-1/secrets-manager/**/*.tfvars'
      - 'apps-devstg/us-east-1/secrets-manager/**/*.hcl'
  issue_comment:
    types: [created]

permissions:
  contents: read       # Required to read repository contents
  pull-requests: write # Required to post PR comments
  issues: write        # Required for @aibot interactions
  models: read         # Required for GitHub Models API

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  AWS_REGION: us-east-1

jobs:
  # Job 1: Detect affected layers from changed files
  detect-layers:
    name: Detect Modified Layers
    runs-on: ubuntu-latest
    outputs:
      layers: ${{ steps.parse-layers.outputs.layers }}
      has-changes: ${{ steps.parse-layers.outputs.has-changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get changed files and extract layers
        id: parse-layers
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "🔍 Analyzing changed files in PR #${{ github.event.pull_request.number }}"

          # Get list of changed files from PR
          gh api repos/${{ github.repository }}/pulls/${{ github.event.pull_request.number }}/files \
            --jq '.[] | select(.status != "removed") | .filename' > changed_files.txt

          echo "📁 Changed files:"
          cat changed_files.txt

          # Force only secrets-manager layer for targeted testing
          echo "🎯 Forcing specific layer for targeted validation: secrets-manager"
          echo "apps-devstg/us-east-1/secrets-manager" > detected_layers.txt

          # Backup: Extract unique layers from changed infrastructure files (disabled)
          # grep -E '\.(tf|tfvars|hcl)$' changed_files.txt | \
          # while IFS='/' read -r account region layer sublayer rest; do
          #   # Handle both 3-level and 4-level layer structures
          #   if [[ -n "$account" && -n "$region" && -n "$layer" ]]; then
          #     if [[ -n "$sublayer" && -d "$account/$region/$layer/$sublayer" ]]; then
          #       # 4-level structure: account/region/layer/sublayer
          #       echo "$account/$region/$layer/$sublayer"
          #     elif [[ -d "$account/$region/$layer" ]]; then
          #       # 3-level structure: account/region/layer
          #       echo "$account/$region/$layer"
          #     fi
          #   fi
          # done | sort -u > detected_layers.txt

          echo "🎯 Detected layers:"
          cat detected_layers.txt

          # Check if we have any valid layers
          if [[ -s detected_layers.txt ]]; then
            # Convert to JSON array for matrix strategy
            layers_json=$(jq -R -s -c 'split("\n") | map(select(length > 0))' detected_layers.txt)
            echo "layers=${layers_json}" >> $GITHUB_OUTPUT
            echo "has-changes=true" >> $GITHUB_OUTPUT
            echo "✅ Found $(wc -l < detected_layers.txt) layer(s) to validate"
          else
            echo "layers=[]" >> $GITHUB_OUTPUT
            echo "has-changes=false" >> $GITHUB_OUTPUT
            echo "ℹ️ No valid infrastructure layers detected in changes"
          fi

  # Job 2: Validate each layer using Leverage CLI (runs in parallel)
  validate-layers:
    name: Validate Layer
    needs: detect-layers
    if: needs.detect-layers.outputs.has-changes == 'true'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        layer: ${{ fromJson(needs.detect-layers.outputs.layers) }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Setup Python virtual environment and Install Leverage CLI
        run: |
          echo "🐍 Creating Python virtual environment (mirroring local setup)"
          python3 -m venv ~/.leverage-venv
          source ~/.leverage-venv/bin/activate

          echo "📦 Installing latest Leverage CLI in virtual environment"
          pip install --upgrade pip
          pip install leverage

          echo "🔧 Configuring environment for subsequent steps"
          echo "VIRTUAL_ENV=$HOME/.leverage-venv" >> $GITHUB_ENV
          echo "$HOME/.leverage-venv/bin" >> $GITHUB_PATH

          echo "✅ Leverage CLI installed in virtual environment"
          leverage --version


      - name: Configure AWS credentials for Leverage CLI containers
        id: aws-config
        run: |
          # Parse layer information to determine AWS account
          LAYER_PATH="${{ matrix.layer }}"
          ACCOUNT=$(echo "$LAYER_PATH" | cut -d'/' -f1)

          echo "🔑 Configuring AWS credentials for Leverage CLI containers - Account: $ACCOUNT"

          # Simple environment variable setup for Leverage CLI containers
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "AWS_DEFAULT_REGION=${{ env.AWS_REGION }}" >> $GITHUB_ENV
          echo "AWS_REGION=${{ env.AWS_REGION }}" >> $GITHUB_ENV

          # Output account for reference (optional)
          echo "account=$ACCOUNT" >> $GITHUB_OUTPUT

          echo "✅ AWS credentials configured for Leverage CLI containers"

      - name: Parse layer information
        id: layer-info
        run: |
          # Extract layer components
          LAYER_PATH="${{ matrix.layer }}"
          ACCOUNT=$(echo "$LAYER_PATH" | cut -d'/' -f1)
          REGION=$(echo "$LAYER_PATH" | cut -d'/' -f2)
          LAYER_NAME=$(echo "$LAYER_PATH" | cut -d'/' -f3)
          SUBLAYER=$(echo "$LAYER_PATH" | cut -d'/' -f4)

          # For 4-level paths, use sublayer for persona selection
          if [[ -n "$SUBLAYER" ]]; then
            LAYER_FOR_PERSONA="$SUBLAYER"
          else
            LAYER_FOR_PERSONA="$LAYER_NAME"
          fi

          # Determine layer type for persona selection
          case "$LAYER_FOR_PERSONA" in
            security-*|secrets-manager|secrets|base-identities) LAYER_TYPE="security" ;;
            base-network|*-vpn|network-*) LAYER_TYPE="network" ;;
            databases-*) LAYER_TYPE="database" ;;
            k8s-*|*-ecs) LAYER_TYPE="container" ;;
            tools-*|*-jenkins|*-monitoring) LAYER_TYPE="devops" ;;
            *-s3|*-backup|*-storage) LAYER_TYPE="storage" ;;
            *-ec2|*-lambda|*-autoscaling) LAYER_TYPE="compute" ;;
            data-*|*-ml|*-bedrock) LAYER_TYPE="data-analytics" ;;
            *) LAYER_TYPE="infrastructure" ;;
          esac

          echo "account=$ACCOUNT" >> $GITHUB_OUTPUT
          echo "region=$REGION" >> $GITHUB_OUTPUT
          echo "layer-name=$LAYER_NAME" >> $GITHUB_OUTPUT
          echo "layer-type=$LAYER_TYPE" >> $GITHUB_OUTPUT

          echo "🏗️ Validating layer: $LAYER_PATH (type: $LAYER_TYPE)"

      - name: Detect Terraform compatibility
        id: tf-compat
        working-directory: ${{ matrix.layer }}
        run: |
          echo "🔍 Detecting Terraform/OpenTofu compatibility..."

          # Default to OpenTofu for modern layers
          TF_COMMAND="tf"
          TF_BINARY="OpenTofu"

          # Check if config.tf exists and analyze version constraints
          if [[ -f "config.tf" ]]; then
            echo "📄 Found config.tf, analyzing version constraints..."

            # Check Terraform required version
            if grep -q "required_version.*~>.*1\.[0-5]" config.tf; then
              TF_COMMAND="terraform"
              TF_BINARY="Terraform (legacy)"
              echo "🔧 Detected legacy Terraform version constraint"
            elif grep -q "required_version.*~>.*1\.[0-3]" config.tf; then
              TF_COMMAND="terraform"
              TF_BINARY="Terraform (legacy)"
              echo "🔧 Detected very old Terraform version constraint"
            fi

            # Check AWS provider version (additional indicator)
            if grep -q "aws.*~>.*[0-4]\\.[0-9]" config.tf; then
              TF_COMMAND="terraform"
              TF_BINARY="Terraform (legacy)"
              echo "🔧 Detected legacy AWS provider version"
            fi
          elif [[ -f "versions.tf" ]]; then
            echo "📄 Found versions.tf, analyzing version constraints..."

            # Check versions.tf for same patterns
            if grep -q "required_version.*~>.*1\.[0-5]" versions.tf; then
              TF_COMMAND="terraform"
              TF_BINARY="Terraform (legacy)"
              echo "🔧 Detected legacy Terraform version in versions.tf"
            fi

            if grep -q "aws.*~>.*[0-4]\\.[0-9]" versions.tf; then
              TF_COMMAND="terraform"
              TF_BINARY="Terraform (legacy)"
              echo "🔧 Detected legacy AWS provider in versions.tf"
            fi
          else
            echo "⚠️ No config.tf or versions.tf found, using default: $TF_BINARY"
          fi

          # Set environment variable for subsequent steps
          echo "TF_COMMAND=$TF_COMMAND" >> $GITHUB_ENV
          echo "tf-command=$TF_COMMAND" >> $GITHUB_OUTPUT
          echo "tf-binary=$TF_BINARY" >> $GITHUB_OUTPUT

          echo "✅ Selected binary: $TF_BINARY (command: leverage $TF_COMMAND)"

      - name: Setup Leverage environment
        id: leverage-setup
        working-directory: ${{ matrix.layer }}
        run: |
          echo "🔧 Setting up Leverage environment..."

          # Navigate to repository root and create minimal build.env
          REPO_ROOT=$(git rev-parse --show-toplevel)
          cd "$REPO_ROOT"

          # Create minimal build.env configuration
          cat > build.env << EOF
          PROJECT=bb
          MFA_ENABLED=false
          TERRAFORM_IMAGE_TAG=1.9.1-tofu-0.3.0
          EOF

          # Source environment for subsequent steps
          source build.env
          echo "PROJECT=$PROJECT" >> $GITHUB_ENV
          echo "MFA_ENABLED=$MFA_ENABLED" >> $GITHUB_ENV
          echo "TERRAFORM_IMAGE_TAG=$TERRAFORM_IMAGE_TAG" >> $GITHUB_ENV

          echo "✅ Leverage environment configured with image: $TERRAFORM_IMAGE_TAG"

      - name: Setup Docker environment for Leverage CLI (Official Troubleshooting)
        run: |
          echo "🐳 Setting up Docker environment for Leverage CLI (applying official troubleshooting fixes)..."

          # Ensure Docker daemon is accessible (GitHub Actions specific)
          sudo chmod 666 /var/run/docker.sock || echo "Could not modify Docker socket permissions"

          # Apply official Leverage CLI troubleshooting fixes
          echo "🔧 Applying official troubleshooting environment variables..."

          # OFFICIAL FIX: Unset SSH_AUTH_SOCK (from troubleshooting guide)
          unset SSH_AUTH_SOCK
          echo "SSH_AUTH_SOCK=" >> $GITHUB_ENV
          echo "✅ SSH_AUTH_SOCK unset (official troubleshooting fix)"

          # Set Docker environment variables (mirroring local setup + official guidance)
          export DOCKER_HOST=unix:///var/run/docker.sock
          echo "DOCKER_HOST=unix:///var/run/docker.sock" >> $GITHUB_ENV
          echo "✅ DOCKER_HOST set to: $DOCKER_HOST"

          # CREATE MISSING CONFIGURATION FILES (Fix for bind mount error)
          echo "🔧 Creating missing configuration files that Leverage CLI expects to mount..."

          # Create minimal .gitconfig file
          mkdir -p $HOME
          cat > $HOME/.gitconfig << 'EOF'
[user]
	name = GitHub Actions
	email = actions@github.com
[init]
	defaultBranch = main
[safe]
	directory = *
EOF
          echo "✅ Created .gitconfig at $HOME/.gitconfig"

          # Create minimal SSH config directory and files
          mkdir -p $HOME/.ssh
          chmod 700 $HOME/.ssh

          # Create empty SSH config file
          touch $HOME/.ssh/config
          chmod 600 $HOME/.ssh/config

          # Create empty known_hosts file
          touch $HOME/.ssh/known_hosts
          chmod 600 $HOME/.ssh/known_hosts

          echo "✅ Created SSH configuration directory and files"

          # Create empty AWS config directory (if it doesn't exist)
          mkdir -p $HOME/.aws
          touch $HOME/.aws/config
          touch $HOME/.aws/credentials
          chmod 600 $HOME/.aws/config $HOME/.aws/credentials
          echo "✅ Created AWS configuration directory and files"

          # List created files for verification
          echo "📁 Created configuration files:"
          echo "Git config: $(ls -la $HOME/.gitconfig 2>/dev/null || echo 'NOT FOUND')"
          echo "SSH directory: $(ls -la $HOME/.ssh 2>/dev/null || echo 'NOT FOUND')"
          echo "AWS directory: $(ls -la $HOME/.aws 2>/dev/null || echo 'NOT FOUND')"

          # Verify environment variables are set correctly
          echo "🔍 Environment variable verification:"
          echo "SSH_AUTH_SOCK: '$SSH_AUTH_SOCK' (should be empty)"
          echo "DOCKER_HOST: '$DOCKER_HOST'"

          # Test Docker connectivity
          echo "🔍 Testing Docker daemon connectivity..."
          docker info || echo "❌ Docker daemon not accessible"
          docker version || echo "❌ Docker version check failed"

          # Test basic container execution
          echo "🧪 Testing basic container execution..."
          if docker run --rm hello-world; then
            echo "✅ Basic Docker container execution successful"
          else
            echo "❌ Basic Docker container execution failed"
          fi

          echo "✅ Docker environment setup complete with official troubleshooting fixes and configuration files"

      - name: Run Leverage CLI validations with debugging
        id: validation
        working-directory: ${{ matrix.layer }}
        run: |
          echo "🚀 Starting Leverage CLI validations with comprehensive debugging..."

          # Ensure virtual environment is active
          echo "🐍 Activating Python virtual environment..."
          source ~/.leverage-venv/bin/activate

          # Verify virtual environment and Leverage CLI
          echo "📊 Environment verification:"
          echo "Python path: $(which python)"
          echo "Python version: $(python --version)"
          echo "Leverage CLI path: $(which leverage)"
          echo "Leverage CLI version: $(leverage --version)"
          echo "Virtual environment: $VIRTUAL_ENV"
          echo "Working directory: $(pwd)"
          echo "User: $(whoami)"
          echo "Groups: $(groups)"

          # Fix path resolution using absolute paths (official troubleshooting approach)
          echo "🔧 Applying path resolution fixes with absolute paths..."

          # Use GITHUB_WORKSPACE for absolute paths (official guidance)
          REPO_ROOT="$GITHUB_WORKSPACE"
          WORK_DIR="$GITHUB_WORKSPACE/apps-devstg/us-east-1/secrets-manager"

          echo "📍 Path resolution:"
          echo "Repository root: $REPO_ROOT"
          echo "Working directory: $WORK_DIR"
          echo "Current PWD: $(pwd)"
          echo "Realpath PWD: $(realpath .)"

          # Ensure we're in the correct absolute directory
          cd "$WORK_DIR"
          echo "✅ Changed to absolute working directory: $(pwd)"

          # Copy build.env instead of symlinking (avoid mount issues)
          if [[ -f "$REPO_ROOT/build.env" ]]; then
            echo "📋 Copying build.env from repository root (avoiding symlink issues)..."
            cp "$REPO_ROOT/build.env" ./build.env
            echo "✅ build.env copied successfully"
          else
            echo "❌ build.env not found at $REPO_ROOT/build.env"
            ls -la "$REPO_ROOT/" | head -10
          fi

          # Validate all required files exist with absolute paths
          echo "📁 File validation:"
          echo "build.env exists: $(test -f build.env && echo 'YES' || echo 'NO')"
          echo "build.env content:"
          cat build.env || echo "❌ Could not read build.env"

          # Verify configuration file paths
          echo "📋 Configuration file paths:"
          echo "Backend config path: $BACKEND_CONFIG"
          echo "Backend config exists: $(test -f $BACKEND_CONFIG && echo 'YES' || echo 'NO')"
          echo "Account config path: $ACCOUNT_CONFIG"
          echo "Account config exists: $(test -f $ACCOUNT_CONFIG && echo 'YES' || echo 'NO')"

          # Docker and container debugging
          echo "🐳 Docker environment debugging:"
          echo "Docker host: $DOCKER_HOST"
          echo "SSH auth sock: $SSH_AUTH_SOCK"
          echo "Docker socket permissions: $(ls -la /var/run/docker.sock)"

          # Test Docker images
          echo "🖼️ Available Docker images:"
          docker images | head -10 || echo "❌ Could not list Docker images"

          # Container pre-validation tests for bind mounts (following official troubleshooting)
          echo "🧪 Container pre-validation tests for bind mounts..."

          # Test basic Leverage CLI execution
          echo "Testing basic Leverage CLI execution:"
          if timeout 30 leverage --version; then
            echo "✅ Basic Leverage CLI execution successful"
          else
            echo "❌ Basic Leverage CLI execution failed"
          fi

          # Test bind mount with current directory (same pattern Leverage CLI will use)
          echo "🔗 Testing bind mount with current working directory:"
          CURRENT_DIR=$(pwd)
          echo "Current directory for mount test: $CURRENT_DIR"

          if docker run --rm -v "$CURRENT_DIR:/workspace" -w /workspace alpine:latest pwd; then
            echo "✅ Basic bind mount test successful"
          else
            echo "❌ Basic bind mount test failed"
          fi

          # Test bind mount with file listing (verify mount contents)
          echo "🔗 Testing bind mount file access:"
          if docker run --rm -v "$CURRENT_DIR:/workspace" -w /workspace alpine:latest ls -la; then
            echo "✅ Bind mount file listing successful"
          else
            echo "❌ Bind mount file listing failed"
          fi

          # Test specific build.env file access in container
          echo "🔗 Testing build.env access in container:"
          if docker run --rm -v "$CURRENT_DIR:/workspace" -w /workspace alpine:latest cat build.env; then
            echo "✅ build.env accessible in container"
          else
            echo "❌ build.env not accessible in container"
          fi

          # Test common configuration file mounts that Leverage CLI typically uses
          echo "🔗 Testing configuration file mounts (fixing bind mount errors):"

          # Test Git config mount
          echo "Git config mount test:"
          if docker run --rm -v "$HOME/.gitconfig:/home/leverage/.gitconfig:ro" alpine:latest ls -la /home/leverage/.gitconfig; then
            echo "✅ Git config mount successful"
          else
            echo "❌ Git config mount failed - checking if file exists:"
            ls -la "$HOME/.gitconfig" || echo "Git config file does not exist"
          fi

          # Test SSH config mount
          echo "SSH config mount test:"
          if docker run --rm -v "$HOME/.ssh:/home/leverage/.ssh:ro" alpine:latest ls -la /home/leverage/.ssh; then
            echo "✅ SSH config mount successful"
          else
            echo "❌ SSH config mount failed - checking if directory exists:"
            ls -la "$HOME/.ssh" || echo "SSH config directory does not exist"
          fi

          # Test AWS config mount
          echo "AWS config mount test:"
          if docker run --rm -v "$HOME/.aws:/home/leverage/.aws:ro" alpine:latest ls -la /home/leverage/.aws; then
            echo "✅ AWS config mount successful"
          else
            echo "❌ AWS config mount failed - checking if directory exists:"
            ls -la "$HOME/.aws" || echo "AWS config directory does not exist"
          fi

          # Enhanced Docker API debugging with mount inspection
          echo "🔍 Enhanced Docker mount debugging..."

          # Test the exact mount pattern Leverage CLI is likely using
          echo "Testing Leverage CLI mount pattern simulation:"
          docker run --rm \
            -v "$CURRENT_DIR:/workspace" \
            -v "$HOME/.gitconfig:/home/leverage/.gitconfig:ro" \
            -v "$HOME/.ssh:/home/leverage/.ssh:ro" \
            -v "$HOME/.aws:/home/leverage/.aws:ro" \
            -w /workspace \
            alpine:latest \
            sh -c "echo 'Mount test successful'; ls -la /home/leverage/; ls -la /workspace" || \
            echo "❌ Full mount pattern test failed"

          # Test Leverage CLI can access Docker (now with verified mounts)
          echo "Testing Leverage CLI Docker access with verbose output:"
          if timeout 60 leverage --verbose tf version; then
            echo "✅ Leverage CLI can access tf in container"
          else
            echo "❌ Leverage CLI cannot access tf in container"
            echo "🔍 Container debugging:"
            docker ps -a | head -5 || echo "Cannot list containers"
            docker images | grep binbash || echo "No binbash images found"

            # Enhanced mount debugging with Docker API inspection
            echo "🔍 Enhanced mount debugging:"
            echo "Directory permissions: $(ls -la .)"
            echo "Parent directory: $(ls -la ..)"
            echo "Home directory files: $(ls -la $HOME)"

            # Show Docker events for mount errors
            echo "🔍 Recent Docker events (last 10):"
            docker events --since="5m" --until="now" 2>/dev/null | tail -10 || echo "Cannot retrieve Docker events"

            # Test with Docker API debug
            echo "🔍 Docker API debugging - testing container creation:"
            DOCKER_BUILDKIT=0 docker run --rm -v "$CURRENT_DIR:/workspace" alpine:latest ls -la /workspace || echo "Docker API debug test failed"
          fi

          # Initialize validation results
          VALIDATION_RESULTS=""
          VALIDATION_STATUS="success"

          # Function to capture command output with proper error handling
          run_validation() {
            local cmd="$1"
            local description="$2"

            echo "⚡ Running: $description"
            echo "Command: $cmd"

            # Ensure virtual environment is active for each command
            source ~/.leverage-venv/bin/activate

            if output=$(timeout 300 $cmd 2>&1); then
              echo "✅ $description: PASSED"
              VALIDATION_RESULTS="${VALIDATION_RESULTS}## $description\n✅ **PASSED**\n\`\`\`\n${output}\n\`\`\`\n\n"
            else
              echo "❌ $description: FAILED"
              echo "Failed output: $output"
              VALIDATION_RESULTS="${VALIDATION_RESULTS}## $description\n❌ **FAILED**\n\`\`\`\n${output}\n\`\`\`\n\n"
              VALIDATION_STATUS="failed"

              # Additional debugging for failed commands
              echo "🔍 Debugging failed command: $cmd"
              echo "Environment variables:"
              env | grep -E "(DOCKER|LEVERAGE|VIRTUAL|PATH)" | head -10
            fi
          }

          echo "🚀 Running Leverage CLI validations in $(pwd)"

          # Configuration paths
          BACKEND_CONFIG="../../config/backend.tfvars"
          ACCOUNT_CONFIG="../../config/account.tfvars"

          echo "📋 Configuration files:"
          echo "Backend config: $(ls -la $BACKEND_CONFIG 2>/dev/null || echo 'NOT FOUND')"
          echo "Account config: $(ls -la $ACCOUNT_CONFIG 2>/dev/null || echo 'NOT FOUND')"

          # Run format check with verbose logging (official recommendation)
          echo "⚡ Running format check with verbose logging..."
          run_validation "leverage --verbose tf fmt -check" "Terraform Format Check (Verbose)"

          # Backend initialization with fallback and verbose logging
          echo "⚡ Initializing with backend (verbose mode)..."
          source ~/.leverage-venv/bin/activate
          if output=$(timeout 300 leverage --verbose tf init -no-color -backend-config=$BACKEND_CONFIG 2>&1); then
            echo "✅ Backend initialization successful"
            VALIDATION_RESULTS="${VALIDATION_RESULTS}## Terraform Backend Initialization\n✅ **PASSED**\n\`\`\`\n${output}\n\`\`\`\n\n"
            BACKEND_AVAILABLE=true
          else
            echo "⚠️ Backend init failed, trying syntax-only validation..."
            echo "Backend init error output: $output"
            VALIDATION_RESULTS="${VALIDATION_RESULTS}## Terraform Backend Initialization\n⚠️ **FAILED (trying syntax-only validation)**\n\`\`\`\n${output}\n\`\`\`\n\n"

            source ~/.leverage-venv/bin/activate
            if fallback_output=$(timeout 300 leverage --verbose tf init -no-color -backend=false 2>&1); then
              echo "✅ Syntax-only initialization successful"
              VALIDATION_RESULTS="${VALIDATION_RESULTS}## Terraform Syntax Initialization\n✅ **PASSED**\n\`\`\`\n${fallback_output}\n\`\`\`\n\n"
              BACKEND_AVAILABLE=false
            else
              echo "❌ Syntax-only initialization failed"
              echo "Syntax-only error output: $fallback_output"
              VALIDATION_RESULTS="${VALIDATION_RESULTS}## Terraform Syntax Initialization\n❌ **FAILED**\n\`\`\`\n${fallback_output}\n\`\`\`\n\n"
              VALIDATION_STATUS="failed"
              BACKEND_AVAILABLE=false
            fi
          fi

          # Run validation with verbose logging
          echo "⚡ Running validation with verbose logging..."
          run_validation "leverage --verbose tf validate" "Terraform Validation (Verbose)"

          # Generate plan if backend available with verbose logging
          if [[ "$BACKEND_AVAILABLE" == "true" ]]; then
            echo "⚡ Generating execution plan with verbose logging..."
            run_validation "leverage --verbose tf plan -no-color -var-file=$ACCOUNT_CONFIG" "Terraform Plan Generation (Verbose)"
          else
            echo "⚠️ Skipping plan generation (backend not available)"
            VALIDATION_RESULTS="${VALIDATION_RESULTS}## Terraform Plan\n⚠️ **SKIPPED - Missing Dependencies**\n\`\`\`\nBackend initialization failed due to missing remote state dependencies.\nThis is expected for layers that depend on other layers being deployed first.\nSyntax validation was performed successfully.\n\`\`\`\n\n"
          fi

          # Save results for AI analysis
          mkdir -p /tmp/validation-results
          LAYER_FILE_NAME="${{ matrix.layer }}"
          LAYER_FILE_NAME="${LAYER_FILE_NAME//\//-}"  # Replace slashes with dashes
          echo -e "$VALIDATION_RESULTS" > "/tmp/validation-results/${LAYER_FILE_NAME}.md"
          echo "$VALIDATION_STATUS" > "/tmp/validation-results/${LAYER_FILE_NAME}.status"

          echo "status=$VALIDATION_STATUS" >> $GITHUB_OUTPUT

          # Create layer slug for unique artifact naming
          LAYER_SLUG="${{ matrix.layer }}"
          LAYER_SLUG="${LAYER_SLUG//\//-}"  # Replace slashes with dashes
          echo "layer-slug=$LAYER_SLUG" >> $GITHUB_OUTPUT

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        with:
          name: validation-results-${{ steps.validation.outputs.layer-slug }}
          path: /tmp/validation-results/
          retention-days: 1

  # Job 3: AI-powered analysis of validation results (DISABLED FOR TESTING)
  ai-analysis:
    name: AI Analysis
    needs: [detect-layers, validate-layers]
    if: false  # Temporarily disabled to avoid API limits during testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download validation results
        uses: actions/download-artifact@v4
        with:
          pattern: validation-results-*
          path: /tmp/validation-results/
          merge-multiple: true

      - name: Load AI personas configuration
        id: load-personas
        run: |
          # Load AI personas from configuration file
          if [[ -f .github/ai-personas.yml ]]; then
            echo "📚 Loading AI personas configuration"
            cat .github/ai-personas.yml
          else
            echo "⚠️ AI personas configuration not found, using defaults"
          fi

      - name: Analyze validation results with AI
        id: ai-analysis
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "🤖 Starting AI analysis of validation results"

          # Get PR diff for context
          PR_DIFF=$(gh api repos/${{ github.repository }}/pulls/${{ github.event.pull_request.number }}/files \
            --jq '.[] | "File: \(.filename)\nStatus: \(.status)\nChanges: +\(.additions) -\(.deletions)\n---"' | head -c 8000)

          # Collect all validation results
          COMBINED_RESULTS="# Validation Results Summary\n\n"
          OVERALL_STATUS="success"

          for result_file in /tmp/validation-results/*.md; do
            if [[ -f "$result_file" ]]; then
              layer_path=$(basename "$result_file" .md)
              status=$(cat "/tmp/validation-results/${layer_path}.status" 2>/dev/null || echo "unknown")

              if [[ "$status" == "failed" ]]; then
                OVERALL_STATUS="failed"
              fi

              COMBINED_RESULTS="${COMBINED_RESULTS}## Layer: $layer_path (Status: $status)\n"
              COMBINED_RESULTS="${COMBINED_RESULTS}$(cat "$result_file")\n"
            fi
          done

          # Prepare AI prompt (simple format to avoid YAML issues)
          AI_PROMPT_BASE="You are an expert DevOps engineer analyzing Terraform infrastructure changes in a Binbash Leverage Reference Architecture project. Context: Repository ${{ github.repository }}, PR #${{ github.event.pull_request.number }}: ${{ github.event.pull_request.title }}. Architecture: Multi-account AWS with layer-based structure. Please provide: 1. Executive Summary, 2. Key Findings, 3. Potential Issues, 4. Recommendations, 5. Cross-Layer Impact Analysis, 6. Risk Assessment. Focus on infrastructure best practices, security implications, and cross-layer dependencies."

          # Combine prompt with data (truncate if too long)
          FULL_PROMPT="${AI_PROMPT_BASE} PR Changes: $(echo "$PR_DIFF" | head -c 3000) Validation Results: $(echo "$COMBINED_RESULTS" | head -c 3000)"

          # Call GitHub Models API
          echo "🧠 Calling GitHub Models API for analysis..."

          # Create JSON payload and call API
          JSON_PAYLOAD=$(jq -n --arg model "openai/gpt-4.1" --arg content "$FULL_PROMPT" '{"model": $model, "messages": [{"role": "user", "content": $content}], "temperature": 0.3, "max_tokens": 2000}')

          # Make API call with error handling
          echo "Calling GitHub Models API..."
          API_RESPONSE=$(curl -s -w "HTTP_CODE:%{http_code}" -X POST \
            -H "Accept: application/vnd.github+json" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "X-GitHub-Api-Version: 2022-11-28" \
            -H "Content-Type: application/json" \
            -d "$JSON_PAYLOAD" \
            "https://models.github.ai/inference/chat/completions")

          # Extract HTTP code and response body
          HTTP_CODE=$(echo "$API_RESPONSE" | grep -o 'HTTP_CODE:[0-9]*' | cut -d: -f2)
          RESPONSE_BODY=$(echo "$API_RESPONSE" | sed 's/HTTP_CODE:[0-9]*$//')

          # Process API response
          if [[ "$HTTP_CODE" == "200" ]]; then
            AI_CONTENT=$(echo "$RESPONSE_BODY" | jq -r '.choices[0].message.content // "AI analysis parsing failed"')
            echo "✅ AI analysis successful"
          else
            echo "⚠️ AI analysis failed with HTTP code $HTTP_CODE"
            echo "📋 API Response Body: $RESPONSE_BODY"
            echo "📊 Payload Size: $(echo "$JSON_PAYLOAD" | wc -c) characters"

            # Try to extract error message from response
            ERROR_MSG=$(echo "$RESPONSE_BODY" | jq -r '.error.message // .message // "Unknown error"' 2>/dev/null || echo "Unable to parse error")
            AI_CONTENT="AI analysis temporarily unavailable (HTTP $HTTP_CODE: $ERROR_MSG). Validation results are still available above."
          fi

          # Save AI response
          mkdir -p /tmp/ai-analysis
          echo "$AI_CONTENT" > /tmp/ai-analysis/response.md
          echo "$OVERALL_STATUS" > /tmp/ai-analysis/status.txt

          echo "overall-status=$OVERALL_STATUS" >> $GITHUB_OUTPUT

      - name: Upload AI analysis
        uses: actions/upload-artifact@v4
        with:
          name: ai-analysis
          path: /tmp/ai-analysis/
          retention-days: 1

  # Job 4: Post results as PR comments (DISABLED FOR TESTING)
  post-results:
    name: Post AI Analysis Results
    needs: [detect-layers, validate-layers, ai-analysis]
    if: false  # Temporarily disabled to avoid API limits during testing
    runs-on: ubuntu-latest
    steps:
      - name: Download AI analysis
        uses: actions/download-artifact@v4
        with:
          name: ai-analysis
          path: /tmp/ai-analysis/

      - name: Post AI analysis as PR comment
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "💬 Posting AI analysis to PR #${{ github.event.pull_request.number }}"

          # Load AI response
          if [[ -f /tmp/ai-analysis/response.md ]]; then
            AI_RESPONSE=$(cat /tmp/ai-analysis/response.md)
            OVERALL_STATUS=$(cat /tmp/ai-analysis/status.txt)
          else
            AI_RESPONSE="AI analysis was not available for this validation run."
            OVERALL_STATUS="unknown"
          fi

          # Determine status emoji
          case "$OVERALL_STATUS" in
            "success") STATUS_EMOJI="✅" ;;
            "failed") STATUS_EMOJI="❌" ;;
            *) STATUS_EMOJI="⚠️" ;;
          esac

          # Create simple PR comment (building it step by step to avoid YAML issues)
          PR_COMMENT="${STATUS_EMOJI} AI-Powered Infrastructure Validation"
          PR_COMMENT+="\n\n**Overall Status:** \`$OVERALL_STATUS\`"
          PR_COMMENT+="\n\n$AI_RESPONSE"
          PR_COMMENT+="\n\n---"
          PR_COMMENT+="\n**Interactive Commands:** Reply with @aibot commands for additional analysis:"
          PR_COMMENT+="\n- \`@aibot explain security risks\`"
          PR_COMMENT+="\n- \`@aibot blast radius\`"
          PR_COMMENT+="\n- \`@aibot best practices\`"
          PR_COMMENT+="\n\n*Powered by Binbash Leverage + GitHub Models API*"

          # Post comment
          gh api repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments \
            --method POST \
            --field body="$PR_COMMENT"

          echo "✅ AI analysis posted successfully"

  # Job 5: Handle @aibot interactive commands (triggered by issue comments)
  handle-aibot:
    name: Handle @aibot Commands
    runs-on: ubuntu-latest
    if: github.event_name == 'issue_comment' && contains(github.event.comment.body, '@aibot')
    steps:
      - name: Process @aibot command
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          COMMENT_BODY="${{ github.event.comment.body }}"
          echo "🤖 Processing @aibot command: $COMMENT_BODY"

          # Extract command after @aibot
          COMMAND=$(echo "$COMMENT_BODY" | sed -n 's/.*@aibot \([^[:space:]]*\).*/\1/p')

          case "$COMMAND" in
            "explain"|"security"|"blast"|"best")
              # Create prompt based on command
              case "$COMMAND" in
                "explain"|"security")
                  BOT_PROMPT="You are a security expert. Analyze the infrastructure changes in this PR and explain potential security risks, vulnerabilities, and recommended mitigations. Focus on AWS security best practices."
                  ;;
                "cost")
                  BOT_PROMPT="You are a cost optimization expert. Analyze the infrastructure changes and estimate cost impact, potential savings opportunities, and cost-effective alternatives."
                  ;;
                "blast")
                  BOT_PROMPT="You are an infrastructure expert. Analyze cross-layer dependencies and predict the blast radius of these changes on other infrastructure components."
                  ;;
                "best")
                  BOT_PROMPT="You are an AWS Well-Architected Framework expert. Review these changes against the 5 pillars: Security, Reliability, Performance, Cost Optimization, and Operational Excellence."
                  ;;
              esac

              # Create JSON payload for API call
              JSON_PAYLOAD=$(jq -n \
                --arg model "openai/gpt-4.1" \
                --arg content "$BOT_PROMPT" \
                '{
                  "model": $model,
                  "messages": [
                    {
                      "role": "user",
                      "content": $content
                    }
                  ],
                  "temperature": 0.3,
                  "max_tokens": 1500
                }')

              # Call GitHub Models API
              AI_RESPONSE=$(curl -s -X POST \
                -H "Accept: application/vnd.github+json" \
                -H "Authorization: Bearer $GITHUB_TOKEN" \
                -H "X-GitHub-Api-Version: 2022-11-28" \
                -H "Content-Type: application/json" \
                -d "$JSON_PAYLOAD" \
                "https://models.github.ai/inference/chat/completions" | \
                jq -r '.choices[0].message.content // "Analysis unavailable"')

              # Reply to comment
              AIBOT_RESPONSE="🤖 **@aibot response for \`$COMMAND\`:**\n\n$AI_RESPONSE"
              gh api repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/comments \
                --method POST \
                --field body="$AIBOT_RESPONSE"
              ;;
            *)
              AIBOT_HELP="🤖 Available @aibot commands:\n- \`@aibot explain security risks\`\n- \`@aibot blast radius\`\n- \`@aibot best practices\`"
              gh api repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/comments \
                --method POST \
                --field body="$AIBOT_HELP"
              ;;
          esac